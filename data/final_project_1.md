# Итоговый проект №1

### [Назад в Содержание ⤶](/README.md)

## Входные данные
[Файл с данными](https://disk.yandex.ru/d/bhf2M8C557AFVw) в формате `.csv`.
Размер распакованного файла: ~ 300 Мб.  
Количество строк: 590708.  

Данные в файле:  
- ID объекта (например, 590707)  
- Координаты (Широта, Долгота) (например, 55.060199, 32.695577)  
- Год постройки (например, 1953.0)  
- Площадь (например, 585.60)  
- Количество этажей (например, 18)  
- Область и город (например, Смоленская область, Ярцево)  
- Адрес (например, "ул. Братьев Шаршановых, д. 61")  
- Описание объекта (например, "Жилой дом в Ярцево, по адресу ул. Братьев Шаршановых, д. 61, 1953 года постройки, 
под управлением ТСЖ «Шаршановых».")  

## Задачи
В проекте используются **Airflow**, **PySpark** и **Clickhouse**, развернуть *docker compose* предварительно с ними.

### Работа с данными и анализ
1. Загрузите файл данных в _DataFrame_ **PySpark**. Обязательно выведите количество строк.
2. Убедитесь, что данные корректно прочитаны (правильный формат, отсутствие пустых строк).
3. Преобразуйте текстовые и числовые поля в соответствующие типы данных (например, дата, число).
4. Вычислите средний и медианный год постройки зданий.
5. Определите топ-10 областей и городов с наибольшим количеством объектов.
6. Найдите здания с максимальной и минимальной площадью в рамках каждой области.
7. Определите количество зданий по десятилетиям (например, сколько зданий построено в 1950-х, 1960-х и т.д.).

### Загрузка данных в БД
8. Создайте схему таблицы в **ClickHouse**, которая будет соответствовать структуре ваших данных (можно не через **Airflow**).
9. Настройте соединение с **ClickHouse** из скрипта (сделать в **Airflow**).
10. Загрузите обработанные данные из _DataFrame_ в таблицу в **ClickHouse** (в **Airflow**).
11. Выполните _SQL_-скрипт в **Python**, который выведет топ 25 домов, у которых площадь больше 60 кв.м (в **Airflow**).  
**Дополнительно:** Сделать графики на **MatPlotLib** для 5, 6 и 7 пунктов.

## Критерии сдачи проекта
1. **Оформление**. Репозиторий оформлен так, как нужно. А именно есть структура, удобное содержание, читаемый _README_, 
естественно наличие условия задания. Обязательно наличие _docker-compose_. Обязательно наличие скрипта `.py`, код в котором 
обернут в _DAG_.  
2. **Логика**. Все сделано в одном скрипте. Сначала читаем данные с csv, потом преобразуем данные, анализируем, 
загружаем в **ClickHouse**, читаем с **ClickHouse**. Все при этом находится в **Airflow**. Откуда загружать `.csv` –
думаем или гуглим (а лучше оба сразу действия делать).  
3. **Содержание**. Файл c `.csv` хранить в **GIT** нельзя! Добавляйте либо ссылку, либо срез данных.  
4. **Корректность**. Данные нельзя редактировать, файл нельзя обрезать. Результаты аналитики и _SQL_-скрипта должны быть 
правильными.  
